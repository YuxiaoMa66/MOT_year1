---
title: "Analysis"
author: "Group7"
date: '2024-03-05'
output: html_document
---
#Abstract

######################################################
#Report
## 1. Introduction

### 1.1 Business Problem Identification


### 1.2 Translation to Machine Learning Task
Explaining that we are making a Prediction

######################################################
## 2. Data Preparation


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(corrplot)
library(reshape2)
```

```{r}
dta <- read.csv("airline_passenger_satisfaction.csv")
dta_csv <- dta # Saveing the origional dataframe
dta <- dta[ ,-which(colnames(dta) == "ID")]
head(dta)
```

```{r library upload}
library(ggplot2)
```


### 2.1 Exploratory Data Analysis

```{r}
str(dta)
```

```{r}
summary(dta)
```

```{r}
# get the number of missing values in each column
colSums(is.na(dta))
```

```{r}
# see quick info of category values (not including the numeric ones)
sapply(dta[,sapply(dta, is.character)], table)
```

Define the function to visualize the distribution of the variabels.
```{r distribution visulization function}
# using ggplot to visualize the distribution of the non-numeric variables as barplot
pplotbar <- function(df, col) {
  ggplot(df, aes_string(col)) +
    geom_bar(stat="count") +
    geom_text(stat="count", aes(label=after_stat(count)), vjust=1.6, color="white") +
    labs(title = paste("Distribution of", col
                          ), x = col, y = "Frequency")
}

# using ggplot to visualize the distribution of the non-numeric variables as pie chart
pplotpie <- function(df, col) {
    p <- ggplot(df, aes_string(x = factor(1), fill = col)) +
        geom_bar(width = 1, stat = "count") +
        geom_text(aes(label = scales::percent(..count../sum(..count..))),
                            stat = "count", 
                            position = position_stack(vjust = 0.5),
                            format = "percent") + 

        coord_polar(theta = "y") +

        labs(x = NULL, y = NULL, fill = col, title = paste("Distribution of", col)) +

        theme_void() +

        theme(legend.position = "right")

    return(p)
}

# using ggplot to visualize the distribution of the numeric variables as histogram, showing the density and median and mean value
plothist <- function(df, col) {
  mean_val <- mean(df[[col]], na.rm = TRUE)
  median_val <- median(df[[col]], na.rm = TRUE)
  
  ggplot(df, aes_string(x = col)) + 
    geom_histogram(aes(y = ..density..), bins = 15, fill = "#656568", color = "black", alpha = 0.7) +
    geom_density(color = "red", size = 1) +
    geom_vline(xintercept = median_val, color = "red", linetype = "dashed", size = 1) +
    geom_vline(xintercept = mean_val, color = "blue", linetype = "dashed", size = 1) + 
    labs(title = paste("Distribution of", col), x = col, y = "Density") +
    theme_minimal() +
    theme(legend.position = "none") +
    annotate("text", x = median_val, y = Inf, label = paste("Median =", round(median_val, 2)), vjust = 2, color = "red") +
    annotate("text", x = mean_val, y = Inf, label = paste("Mean =", round(mean_val, 2)), vjust = 3, color = "blue") 
}

# using ggplot to visualize the distribution of the numeric variables as boxplot, showing the median and quartiles
pbox <- function(df, col) {
  boxplot(df[,col], main = paste("Boxplot of", col), ylab = col)
  invisible()  
}
```


Show the numeric variables' distribution:
```{r}
names_num <- c('Age', 'Flight.Distance', 'Departure.Delay', 'Arrival.Delay')

for (i in names_num) {
  print(plothist(dta, i))
  print(pbox(dta, i))
}
```



```{r}
names_chr <- c('Gender', 'Customer.Type','Type.of.Travel','Class','Satisfaction')
for (i in names_chr) {
  print(pplotbar(dta, i))
}
```

```{r}
for (i in names_chr) {
  print(pplotpie(dta, i))
}
```

Print the variabels to see the distribution of the services:
```{r}
library(ggplot2)
library(dplyr)
library(patchwork)

services_columns <- colnames(dta)[10:(ncol(dta)-4)] 


plots <- list()


for (col in services_columns) {
  p <- dta %>%
    ggplot(aes(x = .data[[col]])) + 
    geom_bar(stat = "count", fill = "#67676c") +
    geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5, position = position_stack(vjust = 0.5)) +
    labs(title = col, x = NULL, y = NULL) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 0, hjust = 1))
  
  plots[[length(plots) + 1]] <- p
}


plot_combined <- wrap_plots(plots, ncol = 3)

print(plot_combined)

```

```{r}
# 1- create a list for service names and another for total ratings
service_names <- names(dta)
total_ratings <- c()

# 2- loop over service columns
for (col in services_columns) {
  total_ratings[col] <- sum(dta[[col]])
}

# 3- sort services by total rating
sorted_indices <- order(total_ratings)
sorted_service_names <- service_names[sorted_indices]
sorted_total_ratings <- total_ratings[sorted_indices]

# 4- create a dataframe to visualize the sorted ratings
service_rating <- data.frame(Service = sorted_service_names, Total = sorted_total_ratings)
service_rating
```
We can see that the most rated services are "Inflight wifi service", "Baggage.Handling" and "Seat.Comfort"; 
and the least rated services are "In.flight.Wifi.Service", "Ease.of.Online.Booking" and "Gate.Location".

```{r}
summary(dta$Flight.Distance)
```

```{r}
p <- ggplot(dta, aes(x = Class, y = Flight.Distance, fill = Class)) + 
  geom_boxplot() +
    labs(title = "Flight Distance by Class", x = "Class", y = "Flight Distance") +
    theme_minimal() +
    theme(legend.position = "none")

print(p)
```
####################################
#### ###**First heatmap here**##############
```{r}
# You code here

```


### 2.2 Data Cleaning

Data cleaning is the process of identifying erroneous, incomplete, inaccurate or irrelevant portions of data and then modifying, replacing or deleting this dirty data. The purpose of data cleaning is to improve the quality of data to make it more suitable for analysis or modeling.

Check the correlation between Departure Delay and Arrival Delay

```{r}
ggplot(dta, aes(x = Departure.Delay, y = Arrival.Delay)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
    labs(title = "Scatter Plot of Departure Delay and Arrival Delay", x = "Departure Delay", y = "Arrival Delay") +
    theme_minimal()
```

Strongly correlated.

```{r}
# regression model to predict Arrival Delay
lmAD <- lm(Arrival.Delay ~ Departure.Delay, data = dta)
summary(lmAD)
```

Use the regression model to fill the missing values in Arrival Delay

```{r}
# fill missing values in Arrival Delay using the regression model
dta$Arrival.Delay[is.na(dta$Arrival.Delay)] <- predict(lmAD, newdata = dta[is.na(dta$Arrival.Delay),])
```

```{r}
# get the number of missing values in each column
colSums(is.na(dta))
```



### 2.3 Feature Engineering

#### 2.3.1 Standarization 

In the previous section, we have seen that the distribution of Flight.Distance, Departure.Delay, and Arrival.Delay are not normal and have many outliers. We will use Box-Cox transformation to normalize Flight.Distance and arctan transformation to normalize Departure.Delay and Arrival.Delay.

Using Box-Cox Transformation to Normalize Flight.Distance
```{r}
# boxcox transformation
library(MASS)
boxcox(dta$Flight.Distance ~ 1)
# choose lambda as the one that maximizes the log-likelihood
bcFD <- boxcox(dta$Flight.Distance ~ 1, plotit = FALSE)
lambda <- bcFD$x[which.max(bcFD$y)]
print(lambda)
```

```{r}
# transform Flight.Distance using the lambda
dta$bcFD <- (dta$Flight.Distance^lambda - 1) / lambda
summary(dta$bcFD)
```

*bcFD* means the box-cox transformation of Flight.Distance.

```{r}
pbox(dta, "bcFD")
```

Now form the boxplot, we can see that the transformed Flight.Distance that there are no outliers and the distribution. And we scaling the bcFD to 0-1 using min-max scaling.

```{r}
dta$bcFD <- (dta$bcFD - min(dta$bcFD)) / (max(dta$bcFD) - min(dta$bcFD))
summary(dta$bcFD)
```

```{r}
pbox(dta, "bcFD")
```

In the future section, we will use the bcFD as the new variable for Flight.Distance.


```{r}
# see distribution of Departure Delay
pbox(dta, "Departure.Delay")
```

```{r}
summary(dta$Departure.Delay)
```

Define arctan transformation function and scaling it to -1-1
Because the arctan transformation is helpful to normalize the data and reduce the impact of outliers as it compresses the range of the data especially when the data is skewed.

```{r}
# arctan transformation adjusted
adjarctan <- function(x) {
  (2/pi) * atan(x)
}
```

```{r}
dta$adjDD <- adjarctan(dta$Departure.Delay)
summary(dta$adjDD)
```

```{r}
pbox(dta, "adjDD")
```

Now we check the distribution of Arrival Delay

```{r}
summary(dta$Arrival.Delay)
```

```{r}
pbox(dta, "Arrival.Delay")
```

```{r}
# arctan transformation adjusted
dta$adjAD <- adjarctan(dta$Arrival.Delay)
summary(dta$adjAD)
```

```{r}
pbox(dta, "adjAD")
```

Now we have the adjDD and adjAD as the new variables for Departure.Delay and Arrival.Delay. But in the next section, we will check the correlation between adjDD and adjAD. If they are correlated, we will use PCA to reduce the dimensionality of adjDD and adjAD.





```{r}
dta$Age <- (dta$Age - min(dta$Age)) / (max(dta$Age) - min(dta$Age)) #Scaling Age
```




#### 2.3.2 Dimensionality Reduction

##### 2.3.2.1 Dummy variables

```{r dta summary}
summary(dta)
```
We identify that there are 4 non-numerical variables, they will be transformed to "Dummy" variables to be able to compute them.

Splitting variables into different groups of dummy variables

```{r}
type_gender <- unique(dta$Gender)
type_gender
type_cust <- unique(dta$Customer.Type)
type_cust
type_travel <- unique(dta$Type.of.Travel)
type_travel
type_class <- unique(dta$Class)
type_class
type_satis <- unique(dta$Satisfaction)
type_satis
```

gender: 1 for Male, 0 for Female Customer_type:
1 for First-Time, 0 for Returning 
Type_of_travel: 1 for Business, 0 for Personal 
Class_business_isBusiness: 1 for Business, 0 for not Business Class_economy_isEconomy: 1 for Economy, 0 for not Economy Satisfaction_satis: 1 for Satisfied, 0 for Neutral or Dissatisfied

```{r}
# Gender, according to the unique values
dta$Gender_dummy <- ifelse(dta$Gender %in% type_gender[1], 1, 0) #gender: 1 for Male, 0 for Female

# Customer.Type, according to the unique values
dta$Customer_type_dummy <- ifelse(dta$Customer.Type %in% type_cust[1], 1, 0)# Customer_type: 1 for first time; 2 for returning

# Type.of.Travel, according to the unique values
dta$Type_of_travel_dummy <- ifelse(dta$Type.of.Travel %in% type_travel[1], 1, 0)# Type_of_travel: 1 for buis; 0 for personal

# Class, according to the unique values (3 classes so 2 dummy variables)
dta$Class_business_dummy <- ifelse(dta$Class %in% type_class[1], 1, 0) #Class_business: 1 for business, 0 for not Business
dta$Class_economy_dummy <- ifelse(dta$Class %in% type_class[2], 1, 0)#Class_economy_isEconomy: 1 for Economy; 0 for not Economy

# Satisfaction, according to the unique values
dta$Satisfaction_dummy <- ifelse(dta$Satisfaction %in% type_satis[2], 1, 0)#Satisfaction_satis: 1 for satisfied,; 0 for Neutral or Dissatisfied


```

```{r}
dta <- dta[, !names(dta) %in% c("Gender", "Customer.Type", "Type.of.Travel", "Class", "Satisfaction")]
#Removing non-numerical collums
#dta <- dta[ ,-which(colnames(dta) == c("Gender", "Customer.Type", "Type.of.Travel", "Class", "Satisfaction"))]

```




##### 2.3.2.2 Principal Component Analysis (PCA)


```{r scaling all variabels}
# If applicable create a forloop that scalesthe rest of the variables, using a c("ALL COL NAMES with scale 1-5")
```


Now, check the adjDD and adjAD to see whether they are correlted with each other

```{r}


dta_loading <- prcomp(dta, scale = TRUE)
summary(dta_loading)


```

Cut off at 90% PC16

```{r}
library("MASS")
library("factoextra")# This library is needed to run it 
fviz_eig(dta_loading, addlables= TRUE, ylim= c(0, 70))# Graph showing proportion of variance
```

```{r}
fviz_pca_biplot(dta_loading) #graph showing directions or smthing
```


```{r}
# draw scatter plot with regression line
ggplot(dta, aes(x = adjDD, y = adjAD)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
    labs(title = "Scatter Plot of adjDD and adjAD", x = "adjDD", y = "adjAD") +
    theme_minimal()
```

```{r}
cor(dta$adjAD, dta$adjDD)

```

Still multicollinearity, so we can use PCA between adjDD and adjAD

```{r}
# PCA
pca <- prcomp(dta[,c("adjDD", "adjAD")], scale = TRUE)
summary(pca)
```

We can see that the first principal component explains 82.14% of the variance, so we can use the first principal component as the new variable.


```{r}
# choose the first principal component
dta$PCDelay <- pca$x[,1]
summary(dta$PCDelay)
```

```{r}
# scaling PCDelay to 0-1
dta$PCDelay <- (dta$PCDelay - min(dta$PCDelay)) / (max(dta$PCDelay) - min(dta$PCDelay))
summary(dta$PCDelay)
```

####**Second heatmap here**


```{r}
correlation_matrix <- cor(dta) #Create correlation matrix

melted_correlation_matrix <- melt(correlation_matrix) #Melt the correlation matrix
```


```{r heatmap, include=TRUE, error= TRUE}


#Can I remove adjDD, adjAD, and bcFD?

ggplot(data = melted_correlation_matrix, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white",
                       midpoint = 0, limit = c(-1, 1), space = "Lab",
                       name="Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(colour = "darkslategray1", angle = 45, hjust = 1, size = 40),
        axis.text.y = element_text(colour = "lightcoral", angle = 45, hjust = 1, size = 40), # Increase size for Y axis labels as well
        axis.title = element_blank()) +
  geom_text(aes(label = sprintf("%.2f", value)), size = 3, vjust = 1.5) +
  geom_tile(data = subset(melted_correlation_matrix, abs(value) > 0.65),
            color = "lawngreen", size = 4, fill = NA)# Change before final, is ugly af with the green banner, only for easy visalisaton

# Use ggsave to save the plot to a file, specifying the dimensions
ggsave("heatmap.png", width = 40, height = 30, dpi = 600)
#I want to print a table with the correlations, I will update later
#print(if(melted_correlation_matrix))
```


######################################################
## 3. Modelling

```{r}
# All variables
allnames <- colnames(dta)
print(allnames)
```

We keep variables below as the analysis variables

```{r}
# Independent variables that are already cleaned
# Numerical variables
IV1 <- c("Age","bcFD","PCDelay")
# Dummy
IV2 <- c("Gender_dummy","Customer_type_dummy","Type_of_travel_dummy","Class_business_dummy","Class_economy_dummy")
# Categorical variables
IV3 <- c("Departure.and.Arrival.Time.Convenience", 
              "Ease.of.Online.Booking", 
              "Check.in.Service", 
              "Online.Boarding", 
              "Gate.Location", 
              "On.board.Service", 
              "Seat.Comfort", 
              "Leg.Room.Service", 
              "Cleanliness", 
              "Food.and.Drink", 
              "In.flight.Service", 
              "In.flight.Wifi.Service", 
              "In.flight.Entertainment", 
              "Baggage.Handling")

IV <- c(IV1, IV2, IV3)
# Dependent variable
DV <- "Satisfaction_dummy"
```




### 3.1 Model Selection and Rationale

### 3.2 Model 1 Description and Training


#### ####Mihkel's basic decision tree here ####


#### Advanced Decision Tree Building

Consider implementing the algorithm of the decision tree, we can use the 3 different algorithms to compute the importance of the 
variables.

1. Information Gain:
$$ \operatorname{Gain}(D, a)=\operatorname{Entropy}(D)-\sum_{k=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Entropy}\left(D^{v}\right) $$
where $$ \operatorname{Entropy}(D)=-\sum_{k=1}^{|y|} p_{k} \log _{2} p_{k} $$
and $$ \operatorname{Entropy}\left(D^{v}\right)=-\sum_{k=1}^{|y|} p_{k} \log _{2} p_{k} $$
2. gain ratio:
$$\operatorname{Gain\_ ratio}(D, a)=\frac{\operatorname{Gain}(D, a)}{I V(a)}$$
where $$ IV(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|} $$

3. Gini index: For the dataset D, the Gini index is defined as:
$$ \operatorname{Gini}(D)=\sum_{k=1}^{|y|} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}}=1-\sum_{k=1}^{|y|} p_{k}{ }^{2} $$
and the Gini index of attribute a is defined as:
$$ \operatorname{Gini\_index} (D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right) $$
where $D^{v}$ is the subset of D for which attribute a has the vth value.

```{r}
# function to compute the importance of the variables
compute_importance <- function(data, attribute, target) {

  entropy <- function(p) {
    if (all(p == 0)) return(0) 
    -sum(p * log2(p))
  }
  

  gini <- function(p) {
    1 - sum(p^2)
  }
  

  total_entropy <- entropy(table(data[[target]]) / nrow(data))

  total_gini <- gini(table(data[[target]]) / nrow(data))
  

  weighted_entropy <- 0
  weighted_gini <- 0
  split_info <- 0
  
  attribute_levels <- unique(data[[attribute]])
  for (level in attribute_levels) {
    subset <- data[data[[attribute]] == level,]
    level_prob <- nrow(subset) / nrow(data)
    
    subset_entropy <- entropy(table(subset[[target]]) / nrow(subset))
    subset_gini <- gini(table(subset[[target]]) / nrow(subset))
    
    weighted_entropy <- weighted_entropy + level_prob * subset_entropy
    weighted_gini <- weighted_gini + level_prob * subset_gini
    split_info <- split_info + (-level_prob * log2(level_prob))
  }
  
  info_gain <- total_entropy - weighted_entropy
  gain_ratio <- ifelse(split_info == 0, 0, info_gain / split_info) 
  imp_gini_index <- total_gini - weighted_gini
  
  return(list(info_gain = info_gain, gain_ratio = gain_ratio, gini_index = imp_gini_index))
}

```

2 ways to do this, 1st is treating the IV3 as numerical (divide by 5 to scale them between 0 and 1), 
2nd is treating the IV3 as categorical.
##### 1st way

```{r}
# copy the dataframe in case we need to use the original dataframe
dta2 <- dta

# create a new dataframe with the selected variables
dta2 <- dta2[,c(IV1, IV2, IV3, DV)]
head(dta2)

```

```{r}
# compute the importance of the variables
# in this case, we only use the dummy variables to see the importance according to the DV, 
# which means only compute the IV2
imp_info_gain <- sapply(IV2, function(x) compute_importance(dta2, x, DV)$info_gain)
imp_gain_ratio <- sapply(IV2, function(x) compute_importance(dta2, x, DV)$gain_ratio)
imp_gini_index <- sapply(IV2, function(x) compute_importance(dta2, x, DV)$gini_index)
```

```{r}
# plot the importance of the variables, using the 3 different algorithms
par(mfrow = c(3,1))
barplot(imp_info_gain, main = "Importance of the Variables (Information Gain)", col = "lightblue")
barplot(imp_gain_ratio, main = "Importance of the Variables (Gain Ratio)", col = "lightgreen")
barplot(imp_gini_index, main = "Importance of the Variables (Gini Index)", col = "lightcoral")
```

All the 3 algorithms show that gender does not have much importance in predicting the satisfaction, so we can remove it from the analysis.
There for we will have no more than $2^4=16$ leaf nodes, which is not too much.
```{r}
# remove the gender variable
IV2 <- c("Customer_type_dummy","Type_of_travel_dummy","Class_business_dummy","Class_economy_dummy")
# copy the dataframe in case we need to use the original dataframe
dta2 <- dta

# create a new dataframe with the selected variables
dta2 <- dta2[,c(IV1, IV2, IV3, DV)]

# scale the categorical variables to 0-1
dta2[,IV3] <- dta2[,IV3] / 5
# separate the data into training and testing sets
set.seed(123)
train_index <- sample(1:nrow(dta2), 0.8 * nrow(dta2))
train_data <- dta2[train_index,]
test_data <- dta2[-train_index,]
```


And we can see that the most important variable is the Class_business_dummy, so we pick it as the first variable to build the decision tree.
Now we can compute the importance of the other variables using the 3 algorithms

Here is the new Selection Algorithm of Variables:
\begin{itemize}
  \item 1. Compute the importance of the variables using the 3 algorithms.In case 3 algs give different results, we scale each alg's result to 0-1 and take the average of the 3 results. 
  \item 2. Pick the biggest result as the selection critera for this node.
  \item 3. For new sepration, we get the new 2 datasets forthe child nodes, and repeat step 1 and 2 until we have 4 variables to build the decision tree.
\end{itemize}

```{r}
# Selection Algorithm of Variables function
select_var <- function(data, IV, DV) {

  # compute the importance of the variables
  imp_info_gain <- sapply(IV, function(x) compute_importance(data, x, DV)$info_gain)
  imp_gain_ratio <- sapply(IV, function(x) compute_importance(data, x, DV)$gain_ratio)
  imp_gini_index <- sapply(IV, function(x) compute_importance(data, x, DV)$gini_index)
  
  # scale the importance to 0-1
  imp_info_gain <- (imp_info_gain - min(imp_info_gain)) / (max(imp_info_gain) - min(imp_info_gain))
  imp_gain_ratio <- (imp_gain_ratio - min(imp_gain_ratio)) / (max(imp_gain_ratio) - min(imp_gain_ratio))
  imp_gini_index <- (imp_gini_index - min(imp_gini_index)) / (max(imp_gini_index) - min(imp_gini_index))
  
  # take the average of the 3 results
  imp_avg <- (imp_info_gain + imp_gain_ratio + imp_gini_index) / 3
  
  # pick the biggest result as the selection critera for this node
  selected_var <- IV[which.max(imp_avg)]
  
  return(selected_var)
}

# Implement the function
Build_tree <- function(data, IV, DV, depth) {
  variables <- IV

  if (depth == 0) return()
  
  # select the variable if there are more than 1 variable left
  if (length(variables) > 1)
    selected_var <- select_var(data, variables, DV)
  else
    selected_var <- variables[1]
  
  # split the data
  left_data <- data[data[[selected_var]] == 0,]
  right_data <- data[data[[selected_var]] == 1,]
  
  # print the selected variable
  print(paste("Selected Variable:", selected_var))

  # drop the selected variable from the list of variables
  variables <- variables[!variables %in% selected_var]
  
  # # print the importance of the selected variable
  # print(paste("Importance of the Selected Variable:", compute_importance(data, selected_var, DV)))
  
  # print the number of observations in the left and right nodes
  print(paste("This is level:", 4-depth+1))
  print("variables left:")
  print(variables)
  print(paste("Number of Observations in the Left Node:", nrow(left_data)))
  print(paste("Number of Observations in the Right Node:", nrow(right_data)))

  
  # repeat the process for the left and right nodes
  Build_tree(left_data, variables, DV, depth - 1)
  Build_tree(right_data, variables, DV, depth - 1)
}
```

```{r}
# Implement the function
Build_tree(train_data, IV2, DV, 4)
```
This can be considered as a DFS output of the decision tree.(not exactly DFS, but similar to it because we print the path of the tree in a depth-first manner.)
It works but seeing that some leaf nodes have only few observations, so we concider pruning the tree.

Consider the dataset's size is in total 129880, and the train_data is 103904, so we can set the minimum number of observations in a
leaf node to be 2% of the train_data, which is 2078.08, using 2000 as the minimum number of observations in a leaf node.
After pruning, store the leaf nodes in a list, and we can consider the mean of the observations in each leaf node as the
new numerical value and later use it combining with other variables to build a logistic regression model.
```{r}
Build_tree_visualize <- function(data, IV, DV, depth, min_obs) {
  tree <- list()
  
  recursive_build <- function(data, IV, DV, depth, min_obs, path = "") {
    if (nrow(data) < 2*min_obs || depth == 0 || length(IV) == 0) {
      leaf_avg <- mean(data[[DV]], na.rm = TRUE)
      return(list(name = paste(path, ", Avg: ", leaf_avg, sep = ""), leaf = TRUE))
    }
    
    selected_var <- if (length(IV) > 1) select_var(data, IV, DV) else IV[1]
    
    if (!selected_var %in% names(data)) {
      return(NULL)
    }

    left_data <- data[data[[selected_var]] == 0, ]
    right_data <- data[data[[selected_var]] == 1, ]

    left_node <- recursive_build(left_data, IV[!IV %in% selected_var], DV, depth - 1, min_obs, paste(path, selected_var, "=0"))
    right_node <- recursive_build(right_data, IV[!IV %in% selected_var], DV, depth - 1, min_obs, paste(path, selected_var, "=1"))

    return(list(name = path, children = list(left_node, right_node)))
  }

  tree <- recursive_build(data, IV, DV, depth, min_obs, "Root")
  return(tree)
}

leaves <- Build_tree_visualize(train_data, IV2, DV, 4, 2000)

```

```{r}
library(igraph)

collect_edges <- function(node, parent_name = NA) {
  edges <- matrix(nrow = 0, ncol = 2)
  if (!is.na(parent_name) && !is.null(node$name)) {
    edges <- rbind(edges, c(parent_name, node$name))
  }
  if (!is.null(node$children)) {
    for (child in node$children) {
      if (!is.null(child)) { 
        child_edges <- collect_edges(child, node$name)
        edges <- rbind(edges, child_edges)
      }
    }
  }
  return(edges)
}

edges_matrix <- collect_edges(leaves)
g <- graph_from_edgelist(edges_matrix, directed = TRUE)
layout <- layout_as_tree(g)
plot(g, layout = layout, edge.arrow.size=.5, vertex.label.cex=.6, vertex.size=20, vertex.color="lightblue", main="Decision Tree Visualization")


```
From the graph, we can see that the tree has 12 leaf nodes.



```{r}
# Recursive function to build the decision tree and prune it. Store the leaf nodes in a list and return it.
Build_tree_prune <- function(data, IV, DV, depth, min_obs) {
  leaves <- list()
  
  recursive_build <- function(data, IV, DV, depth, min_obs, path = list()) {
    # 检查是否达到了递归的基本条件
    if (nrow(data) < min_obs || depth == 0 || length(IV) == 0) {
      return(list(data = data, path = path)) 
    }
    
    # 选择一个变量进行分裂
    if (length(IV) > 1) {
      selected_var <- select_var(data, IV, DV)
    } else {
      selected_var <- IV[1]
    }
    
    # 按照选定的变量对数据进行分割
    left_data <- data[data[[selected_var]] == 0, ]
    right_data <- data[data[[selected_var]] == 1, ]
    
    # 如果任一子集的大小小于 min_obs，当前节点应被视为叶节点
    if (nrow(left_data) < min_obs || nrow(right_data) < min_obs) {
      return(list(data = data, path = path))
    }

    new_path_left <- c(path, paste(selected_var, "= 0"))
    new_path_right <- c(path, paste(selected_var, "= 1"))
    
    left_leaves <- if (nrow(left_data) > 0) recursive_build(left_data, IV[!IV %in% selected_var], DV, depth - 1, min_obs, new_path_left) else NULL
    right_leaves <- if (nrow(right_data) > 0) recursive_build(right_data, IV[!IV %in% selected_var], DV, depth - 1, min_obs, new_path_right) else NULL

    return(c(left_leaves, right_leaves))
  }
  
  leaves <- recursive_build(data, IV, DV, depth, min_obs)
  
  return(leaves)
}

```

```{r}
# Implement the function
leaves_data <- Build_tree_prune(train_data, IV2, DV, 4, 2000)
```

```{r}
# print the leaf nodes, dropping the NULL values
leaves_data <- leaves_data[!sapply(leaves_data, is.null)]
leaves_data
```
Now we see that the leaf nodes have more than 2000 observations, and we get 7 leaf nodes in total.
  
```{r}
# get train_data_list from leaves_data, which is a list of dataframes and is the odd elements of leaves_data
train_data_list <- leaves_data[sapply(seq(leaves_data), function(x) x %% 2 == 1)]
```
Now we can compute the mean of the observations in each leaf node as the new numerical value and later use it combining with other variables to build a logistic regression model.
```{r}
# Function to compute the mean of the observations in each leaf node
compute_mean <- function(data) {
  mean_value <- mean(data[[DV]], na.rm = TRUE)
  return(mean_value)
}

```
```{r}
# compute the mean of the observations in each leaf node and store each mean in the data's new column named "leaf_mean"
for (i in seq(length(train_data_list))) {
  train_data_list[[i]]$leaf_mean <- compute_mean(train_data_list[[i]])
}
```
```{r}

```


In case in the future we forget to adjust the path for the testing data, we can define a function to adjust the path for the testing data.

Now we add the new variable "leaf_mean" to the testing data by using the mean of the observations in each leaf node.

```{r}
# Function to add the new variable "leaf_mean" to the testing data's dataframe column by using the mean_value from the training data's leaf node
# which means for the same path, we use the same mean_value for the testing data in the column "leaf_mean"
add_leaf_mean <- function(test_data, train_data_list) {
  test_data$leaf_mean <- NA
  
  for (i in seq(length(train_data_list))) {
    conditions <- leaves_data[2*i]$path
    mean_value <- train_data_list[[i]]$leaf_mean
    
    for (condition in conditions) {
      parts <- strsplit(condition, " = ")[[1]]
      variable <- parts[1]
      value <- as.numeric(parts[2])
      
      test_data$leaf_mean[test_data[[variable]] == value] <- mean_value
    }
  }
  
  return(test_data)
}

```

```{r warning=FALSE}
# implement the function
train_data_new <- add_leaf_mean(train_data, train_data_list)
test_data_new <- add_leaf_mean(test_data, train_data_list)
```

```
```{r}
# # Function to adjust the path for the testing data

# split_test_data <- function(test_data, leaves_data) {
#   split_datasets <- list()

#   len <- length(leaves_data)
  
#   for (i in seq(len/2)) {

#     current_subset <- test_data
    

#     conditions <- leaves_data[2*i]$path
#     # print(conditions)
    
#     for (condition in conditions) {

#       parts <- strsplit(condition, " = ")[[1]]
#       variable <- parts[1]
#       value <- as.numeric(parts[2])
      

#       current_subset <- current_subset[current_subset[[variable]] == value, ]
#     }
    

#     split_datasets[[i]] <- current_subset
#   }
  
#   return(split_datasets)
# }

# test_data_new <- split_test_data(test_data_new, leaves_data)

# for (i in seq(length(test_data_new))) {
#   print(paste("Number of Observations in Leaf Node", i, ":", nrow(test_data_new[[i]])))
# }
```

Drop the IV2 from the training and testing data, and we can build the logistic regression model using the new variables.

```{r}
# drop the IV2 from the training and testing data
train_data_new <- lapply(train_data_new, function(x) x[, !names(x) %in% IV2])
test_data_new <- lapply(test_data_new, function(x) x[, !names(x) %in% IV2])
```

Logsitic Regression Model
$$ \log \left(\frac{p}{1-p}\right)=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\ldots+\beta_{n} x_{n} $$
where $p$ is the probability of the event that the dependent variable is 1, and $x_{1}, x_{2}, \ldots, x_{n}$ are the independent variables.

Explaining the logistic regression model:
- The left-hand side of the equation is the log-odds of the dependent variable being 1.
- The right-hand side of the equation is the linear combination of the independent variables.
- The coefficients $\beta_{0}, \beta_{1}, \beta_{2}, \ldots, \beta_{n}$ are the parameters of the model that need to be estimated.
- The logistic function is used to transform the log-odds to the probability of the dependent variable being 1.

For the logistic regression model, we use the binomial family and the logit link function.
In addion,In order to solve the issue arising in (f), we add a ridge penalty to the log-likelihood criterion:
$$J(\boldsymbol{\beta})=-\ell(\boldsymbol{\beta})+\lambda \sum_{j=1}^{p} \beta_{j}^{2}$$
where $\ell(\boldsymbol{\beta})$ is the log-likelihood criterion, $\lambda$ is the penalty parameter, and $p$ is the number of parameters in the model.
where we take $\lambda$ = 1 and $\ell$($\beta$) denotes the log-likelihood function of the logistic regression
model. Adapt the Newton-Raphson algorithm such that we obtain the regularised estimator
of $\beta$ that minimizes J($\beta$).

Newton-Raphson algorithm:
1. Initialize $\beta^{(0)}$.
2. For $k=0,1,2, \ldots$ until convergence:
    - Compute the gradient vector $\nabla J\left(\beta^{(k)}\right)$.
    - Compute the Hessian matrix $H\left(\beta^{(k)}\right)$.
    - Update $\beta^{(k+1)}=\beta^{(k)}-H^{-1}\left(\beta^{(k)}\right) \nabla J\left(\beta^{(k)}\right)$.
    - Check for convergence.
    - If converged, stop; otherwise, go to step 2.
    - The convergence criterion can be based on the change in the log-likelihood function or the change in the parameter estimates.
- 

```{r}
# Function to build the logistic regression model
library(glmnet)

train_and_evaluate_models <- function(train_data_list, test_data_list, target_column_name, lambda = 1) {
  if (length(train_data_list) != length(test_data_list)) {
    stop("The number of training and testing datasets must be the same.")
  }
  
  accuracy_list <- numeric(length(train_data_list))
  
  for (i in seq_along(train_data_list)) {
    train_data <- train_data_list[[i]]
    test_data <- test_data_list[[i]]
    
    y_train <- train_data[[target_column_name]]
    x_train <- as.matrix(train_data[, setdiff(names(train_data), target_column_name)])
    
    y_test <- test_data[[target_column_name]]
    x_test <- as.matrix(test_data[, setdiff(names(test_data), target_column_name)])
    
    # 训练模型，使用L2正则化
    model <- glmnet(x_train, y_train, family = "binomial", alpha = 0, lambda = lambda)
    
    # 预测测试数据集
    predictions <- predict(model, newx = x_test, type = "class")
    
    # 计算并存储准确率
    accuracy_list[i] <- mean(predictions == y_test)
    
    # 打印子数据集的准确率
    cat("Accuracy for dataset", i, ":", accuracy_list[i], "\n")
  }
  
  # 计算并打印整体准确率
  overall_accuracy <- mean(accuracy_list)
  cat("Overall accuracy:", overall_accuracy, "\n")
  
  return(overall_accuracy)
}


```

```{r}
# implement the function
accuracy <- train_and_evaluate_models(train_data_new, test_data_new, "Satisfaction_dummy", lambda = 1)
```

```{r}
```

```{r}
# Function to compute the accuracy of the logistic regression model
compute_accuracy <- function(data) {
  data$prediction <- ifelse(data$prediction > 0.5, 1, 0)
  accuracy <- sum(data$prediction == data$Satisfaction_dummy) / nrow(data)
  return(accuracy)
}

```

```{r}
# compute the accuracy of the logistic regression model
accuracy <- sapply(model_results, compute_accuracy)
accuracy
```

```{r}
```

```{r}
```

```{r}
```



```{r}
```

### 3.3 Model 2 Description and Training

### 3.4 Model Refinement

#####################################################
## 4. Results and discussion 

### 4.1 Model Comparison 

######################################################
## 5. Conclusion and future work

### 5.1 Key Findings and Insights

### 5.2 Recommendations for Business Action

### 5.3 Limitations and Future Directions


######################################################
# References

#Appendices